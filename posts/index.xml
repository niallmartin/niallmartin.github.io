<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Niall Martin</title>
    <link>https://niallmartin.io/posts/</link>
    <description>Recent content in Posts on Niall Martin</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 05 Dec 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://niallmartin.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Analysing and Predicting Stock Market Trends</title>
      <link>https://niallmartin.io/posts/analysing-and-predicting-stock-market-trends/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://niallmartin.io/posts/analysing-and-predicting-stock-market-trends/</guid>
      <description>Aim Put simply, this notebook aims to analyse stock-market data and demonstrate how to implement a LSTM RNN model using the Keras and Tensorflow. A high level summary of the results is outline in this post.
A link to the main colab notebook is available here.
Notes on the data Data was downloaded from Yahoo finance, which includes the following features:
 Opening share price at the start of each trading day.</description>
    </item>
    
    <item>
      <title>Ongoing List of my Favourite Data Science Blogs</title>
      <link>https://niallmartin.io/posts/ongoing-list-of-my-favourite-data-science-blogs/</link>
      <pubDate>Tue, 15 Oct 2019 15:15:12 +0000</pubDate>
      
      <guid>https://niallmartin.io/posts/ongoing-list-of-my-favourite-data-science-blogs/</guid>
      <description>&amp;ldquo;Life is really simple, but we insist on making it complicated&amp;rdquo; ~ Confucius
Simple is harder than complex. And this&#39;s particularly true of academic research and ML models. I&#39;m always on the lookout for resources that aim to simply describe statistical concepts and machine learning in an simple manner. Those who can do so truly understand the topic.
Here&#39;s an ongoign list of blogs and resources that I love the most.</description>
    </item>
    
    <item>
      <title>How to Start Learning Deep Learning</title>
      <link>https://niallmartin.io/posts/how-to-start-learning-deep-learning/</link>
      <pubDate>Wed, 23 May 2018 10:58:32 +0000</pubDate>
      
      <guid>https://niallmartin.io/posts/how-to-start-learning-deep-learning/</guid>
      <description>The Basics Due to the recent achievements of artificial neural networks across many different tasks (such as face recognition, object detection and Go), deep learning has become extremely popular. This post aims to be a starting point for those interested in learning more about it.
If you already have a basic understanding of linear algebra, calculus, probability and programming:
I recommend starting with Stanford’s CS231n. The course notes are comprehensive and written well.</description>
    </item>
    
    <item>
      <title>Introductory Resources for Data Science</title>
      <link>https://niallmartin.io/posts/introductory-resources-for-data-science/</link>
      <pubDate>Wed, 24 May 2017 10:58:32 +0000</pubDate>
      
      <guid>https://niallmartin.io/posts/introductory-resources-for-data-science/</guid>
      <description>Getting started in data science can be a daunting task. The area is broad, with little guidance on where one topic start and the next ends. Because of that, I&#39;ve pulled together this list of resources that particularly helped me when I was starting out.
I&#39;ve broken the links into the following four categories, each of which distinguishes between the different skills required data scientists:
 Data Analytics - Involves feature engineering and statistical tests.</description>
    </item>
    
    <item>
      <title>The Simpler Derivation of Logistic Regression</title>
      <link>https://niallmartin.io/posts/the-simpler-derivation-of-logistic-regression/</link>
      <pubDate>Wed, 11 Jan 2017 10:58:32 +0000</pubDate>
      
      <guid>https://niallmartin.io/posts/the-simpler-derivation-of-logistic-regression/</guid>
      <description>Main Derivation Logistic regression is one of the most popular ways to fit models for categorical data, especially for binary response data. It is the most important (and probably most used) member of a class of models called generalized linear models. Unlike linear regression, logistic regression can directly predict probabilities (values that are restricted to the (0,1) interval); furthermore, those probabilities are well-calibrated when compared to the probabilities predicted by some other classifiers, such as Naive Bayes.</description>
    </item>
    
    <item>
      <title>Classification Model to Predict Loan Default</title>
      <link>https://niallmartin.io/posts/classification-model-to-predict-loan-default/</link>
      <pubDate>Fri, 19 Aug 2016 00:00:00 +0100</pubDate>
      
      <guid>https://niallmartin.io/posts/classification-model-to-predict-loan-default/</guid>
      <description>As part of my Insight project, I choose to help Zidisha improve their default detection system. But wait, who is Zidisha?
Zidisha&amp;hellip; Zidisha is a non-profit startup that allows entrepreneurs in developing countries to request credit. Using a crowdsourcing financial model, individuals like Bineta are able to raise the necessary funds to grow their business. This approach allows entrepreneurs to develop sustainable businesses, and provides jobs for additional members in the community.</description>
    </item>
    
    <item>
      <title>Building Classification Models</title>
      <link>https://niallmartin.io/posts/building-classification-models/</link>
      <pubDate>Thu, 11 Aug 2016 00:00:00 +0100</pubDate>
      
      <guid>https://niallmartin.io/posts/building-classification-models/</guid>
      <description>Building Building on information presented in my previous post, this articles describes how bagging and boosted can be used to build a classification model. The data is available from Kaggle, and represents the financial transactions from two hundred thousand individuals (each with 800 features).
Using this dataset, I set about building a loan-default model. The overall pipeline for this project was split into a few stages, all of which are described below.</description>
    </item>
    
  </channel>
</rss>